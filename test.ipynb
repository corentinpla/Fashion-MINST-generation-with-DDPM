{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "import random\n",
    "import IPython\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_path, batch_size, test = False):\n",
    "    \n",
    "    dataset = load_dataset(data_path)\n",
    "\n",
    "    # define image transformations (e.g. using torchvision)\n",
    "    transform = Compose([\n",
    "        transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "        transforms.ToTensor(),  # Transform PIL image into tensor of value between [0,1]\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)  # Normalize values between [-1,1], plus pratique pour entrainer\n",
    "    ])\n",
    "\n",
    "    # define function for HF dataset transform\n",
    "    def transforms_im(examples):\n",
    "        examples['pixel_values'] = [transform(image) for image in examples['image']]\n",
    "        del examples['image']\n",
    "        return examples\n",
    "\n",
    "    dataset = dataset.with_transform(transforms_im).remove_columns('label')  # We don't need it \n",
    "    channels, image_size, _ = dataset['train'][0]['pixel_values'].shape\n",
    "        \n",
    "    if test:\n",
    "        dataloader = DataLoader(dataset['test'], batch_size=batch_size)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    len_dataloader = len(dataloader)\n",
    "    print(f\"channels: {channels}, image dimension: {image_size}, len_dataloader: {len_dataloader}\")  \n",
    "    \n",
    "    return dataloader, channels, image_size, len_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab9821ecc98de011741dfaec3c017883b962b1388b9630c30169ab308e0d1578"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
